{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Panel Estimations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from numpy import linalg as la\n",
    "from scipy.stats import chi2\n",
    "from tabulate import tabulate\n",
    "import LinearModelsProject1 as lm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"firms.csv\")\n",
    "N_list = data.firmid.unique()\n",
    "T_list = data.year.unique()\n",
    "\n",
    "N = data.firmid.unique().size\n",
    "T = data.year.unique().size\n",
    "\n",
    "y = data.ldsa.values.reshape((N*T,1))\n",
    "l = data.lemp.values.reshape((N*T,1))\n",
    "k = data.lcap.values.reshape((N*T,1))\n",
    "\n",
    "\n",
    "constant = np.ones((y.shape[0], 1))\n",
    "X = np.hstack([constant, l, k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooled Ordinary Least Squares (POLS)\n",
    "\n",
    "We estimate the parameters in the following econometric model:\n",
    "\n",
    "$\\begin{equation} \\log y_{it} = \\beta_K\\log k_{it} + \\beta_L \\log \\ell_{it} + \\log A_i + \\log A_{it} \\end{equation}$\n",
    "\n",
    "Using the method of POLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled OLS\n",
      "Dependent variable: Log Deflated Sales\n",
      "\n",
      "                        Beta      Se    t-values\n",
      "--------------------  ------  ------  ----------\n",
      "c                     0.0000  0.0050      0.0000\n",
      "Log Employment        0.6748  0.0102     66.4625\n",
      "Log Adjusted Capital  0.3100  0.0091     33.9237\n",
      "R² = 0.914\n",
      "σ² = 0.131\n"
     ]
    }
   ],
   "source": [
    "## Assigning label names for the regressors and the dependent variable\n",
    "\n",
    "label_x = [\"c\", \"Log Employment\", \"Log Adjusted Capital\"]\n",
    "label_y = \"Log Deflated Sales\"\n",
    "\n",
    "## Using the POLS method to estimate the parameters\n",
    "\n",
    "ols_result = lm.estimate(y, X)\n",
    "\n",
    "## Print the table containing estimation data\n",
    "\n",
    "lm.print_table(\n",
    "    (label_y, label_x), ols_result, title=\"Pooled OLS\", floatfmt='.4f'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Effects Method (FE)\n",
    "\n",
    "The fixed effects model is written as:\n",
    "\n",
    "$$ \\mathbf{\\ddot{y}}_{it} = \\mathbf{\\ddot{X}}_{it}\\beta + \\mathbf{\\ddot{u}}_{it} \\tag{2}$$\n",
    "\n",
    "Where the observations have been time-demeaned to get rid of the fixed effects. This is done by subtracting the time average of the observations, mathematically expressed as:\n",
    "\n",
    "$$ \\ddot{y}_{it} = y_{it} - \\bar{y}_i\\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE regression\n",
      "Dependent variable: Log Deflated Sales\n",
      "\n",
      "                        Beta       Se    t-values\n",
      "--------------------  ------  -------  ----------\n",
      "Log Employment        0.6942  0.04165      16.67\n",
      "Log Adjusted Capital  0.1546  0.02995       5.163\n",
      "R² = 0.477\n",
      "σ² = 0.018\n"
     ]
    }
   ],
   "source": [
    "## Defining the demeaning matrix\n",
    "\n",
    "def demeaning_matrix(T):\n",
    "    Q_T = np.eye(T) - np.tile(1/T, (T, T))\n",
    "    return Q_T\n",
    "\n",
    "Q_T = demeaning_matrix(T)\n",
    "\n",
    "## Apply Q_T on the observables\n",
    "\n",
    "y_demean = lm.perm(Q_T, y)\n",
    "x_demean = lm.perm(Q_T, X)\n",
    "\n",
    "# Deleting the zero columns and updating the list of regressor labels\n",
    "\n",
    "x_demean = x_demean[:, 1:]\n",
    "label_x_fe = label_x[1:]\n",
    "\n",
    "# Estimate using OLS for the FE method\n",
    "\n",
    "fe_result = lm.estimate(\n",
    "    y_demean, x_demean, transform='fe', T=T, robust_se=True\n",
    ")\n",
    "\n",
    "# Print the table containing estimation data\n",
    "\n",
    "lm.print_table(\n",
    "    (label_y,label_x_fe), \n",
    "    fe_result, title='FE regression', floatfmt='.4'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Differencing Method (FD)\n",
    "\n",
    "We can write the FD-transformed regression model as:\n",
    "\n",
    "$$ \\Delta y_{it} = \\Delta \\mathbf{X}_{it}+\\Delta u_{it} \\tag{4}$$\n",
    "\n",
    "Where the FD-transformation is denoted as:\n",
    "\n",
    "$$ \\Delta y_{it} = y_{it}-y_{it-1} \\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD regression\n",
      "Dependent variable: Log Deflated Sales\n",
      "\n",
      "                        Beta      Se    t-values\n",
      "--------------------  ------  ------  ----------\n",
      "Log Employment        0.5487  0.0284     19.3056\n",
      "Log Adjusted Capital  0.0630  0.0229      2.7460\n",
      "R² = 0.165\n",
      "σ² = 0.014\n"
     ]
    }
   ],
   "source": [
    "## Defining the FD matrix \n",
    "\n",
    "def fd_matrix(T):\n",
    "    D_T = np.eye(T) - np.eye(T, k=-1)\n",
    "    D_T = D_T[1:]\n",
    "    return D_T\n",
    "\n",
    "D_T = fd_matrix(T)\n",
    "\n",
    "## Applying the FD matrix on the observables\n",
    "\n",
    "y_diff = lm.perm(D_T, y)\n",
    "x_diff = lm.perm(D_T, X[:,1:])\n",
    "\n",
    "# Estimate using OLS for the first differencing method\n",
    "\n",
    "fd_result = lm.estimate(y_diff, x_diff, transform='fd', T=T, robust_se=True)\n",
    "\n",
    "# Print the table containing estimation data\n",
    "\n",
    "lm.print_table(\n",
    "    (label_y, label_x[1:]), \n",
    "    fd_result, title='FD regression', floatfmt='.4f'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Effects Method (RE)\n",
    "\n",
    "Random effects provides another approach of handling the unobserved heterogeneity. Instead of removing $c_i$ from the composite error term, we create a quasi-demeaned regression model denoted the following way:\n",
    "\n",
    "$$ \\check{y}_{it}=\\mathbf{\\check{X}}_{it}+\\check{v}_{it}\\tag{6}$$\n",
    "\n",
    "Where the quasi-demeaned observables are obtained by:\n",
    "\n",
    "$$ \\check{y}_{it} = y_{it}-\\hat{\\lambda}\\bar{y}_i \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RE\n",
      "Dependent variable: Log Deflated Sales\n",
      "\n",
      "                        Beta      Se    t-values\n",
      "--------------------  ------  ------  ----------\n",
      "c                     0.0000  0.0168        0.00\n",
      "Log Employment        0.7197  0.0335       21.46\n",
      "Log Adjusted Capital  0.1989  0.0261        7.62\n",
      "R² = 0.642\n",
      "σ² = 0.018\n",
      "λ = 0.887\n"
     ]
    }
   ],
   "source": [
    "## Defining the matrix for between estimation\n",
    "\n",
    "def mean_matrix(T):\n",
    "    return np.tile(1/T, (1, T))\n",
    "P_T = mean_matrix(T)\n",
    "\n",
    "## Applying the mean matrix on the observables\n",
    "\n",
    "y_mean = lm.perm(P_T, y)\n",
    "x_mean = lm.perm(P_T, X)\n",
    "\n",
    "## Estimating parameter values using the between estimation method\n",
    "\n",
    "be_result = lm.estimate(\n",
    "    y_mean, x_mean, transform='be', robust_se=True)\n",
    "\n",
    "# Defining sigma squared values to obtain lambda\n",
    "\n",
    "sigma_u = fe_result['sigma2']\n",
    "sigma_c = be_result['sigma2'] - sigma_u/T\n",
    "_lambda = 1 - np.sqrt(sigma_u/(sigma_u + T*sigma_c))\n",
    "\n",
    "# Defining the quasi-demeaning matrix\n",
    "\n",
    "C_t = np.eye(T) - _lambda*mean_matrix(T)\n",
    "\n",
    "# Applying the matrix on the observables\n",
    "\n",
    "x_re = lm.perm(C_t, X)\n",
    "y_re = lm.perm(C_t, y)\n",
    "\n",
    "# Estimating using OLS with the random effects method\n",
    "\n",
    "re_result = lm.estimate(\n",
    "    y_re, x_re, transform='re', T=T, robust_se=True\n",
    ")\n",
    "\n",
    "# Printing a table containing estimation data\n",
    "\n",
    "lm.print_table(\n",
    "    labels=(label_y, label_x), results=re_result, _lambda=_lambda,\n",
    "    title='RE',\n",
    "    floatfmt=['', '.4f', '.4f', '.2f']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SERIAL CORRELATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in comparing the FE and FD methods. This can be done by a serial correlation test. The following algorithm is used to check if there are any observed serial correlation in the idiosyncratic error term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updating the list of year to not only contain the unique values\n",
    "\n",
    "T_list = data.year.values\n",
    "\n",
    "## Filtering the list to exclude the even-year observations\n",
    "\n",
    "T_list = np.delete(T_list, np.s_[::2], axis=0)\n",
    "\n",
    "## Removing the first-year observations\n",
    "\n",
    "reduced_T_list = T_list[T_list != 1969]\n",
    "\n",
    "## Defining the function for serial correlation testing\n",
    "\n",
    "def serial_corr(y, X, T, T_list):\n",
    "\n",
    "    ## Defining estimated parameters and residuals\n",
    "\n",
    "    b_hat = lm.est_ols(y, X)\n",
    "    e = y - X@b_hat\n",
    "    \n",
    "    ## Defining the lag matrix\n",
    "\n",
    "    L_T = np.eye(T, k=-1)\n",
    "    L_T = L_T[1:]\n",
    "\n",
    "    ## Applying the lag matrix\n",
    "    \n",
    "    e_l = lm.perm(L_T, e)\n",
    "    \n",
    "    ## Removing the first observation for every firm\n",
    "\n",
    "    e = e[T_list != 1971]\n",
    "\n",
    "    return lm.estimate(e, e_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along axis 0; size of axis is 4851 but size of corresponding boolean axis is 2205",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Generating serial correlation answers for FD and FE estimations\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m corr_result_fd \u001b[38;5;241m=\u001b[39m \u001b[43mserial_corr\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_diff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_diff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduced_T_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m corr_result_fe \u001b[38;5;241m=\u001b[39m serial_corr(y_demean, x_demean, T, T_list)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m## Defining label and title names\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 33\u001b[0m, in \u001b[0;36mserial_corr\u001b[0;34m(y, X, T, T_list)\u001b[0m\n\u001b[1;32m     29\u001b[0m e_l \u001b[38;5;241m=\u001b[39m lm\u001b[38;5;241m.\u001b[39mperm(L_T, e)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m## Removing the first observation for every firm\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mT_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1971\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mestimate(e, e_l)\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along axis 0; size of axis is 4851 but size of corresponding boolean axis is 2205"
     ]
    }
   ],
   "source": [
    "## Generating serial correlation answers for FD and FE estimations\n",
    "\n",
    "corr_result_fd = serial_corr(y_diff, x_diff, T-1, reduced_T_list)\n",
    "\n",
    "corr_result_fe = serial_corr(y_demean, x_demean, T, T_list)\n",
    "\n",
    "## Defining label and title names\n",
    "\n",
    "label_ye = 'OLS residual, e\\u1d62\\u209c'\n",
    "label_e = ['e\\u1d62\\u209c\\u208B\\u2081']\n",
    "title = 'Serial Correlation'\n",
    "\n",
    "## Printing the table for FD\n",
    "\n",
    "lm.print_table(\n",
    "    (label_ye, label_e), corr_result_fd, \n",
    "    title='Serial Correlation for First Differencing', floatfmt='.4f'\n",
    ")\n",
    "\n",
    "## Printing the table for FE\n",
    "\n",
    "lm.print_table(\n",
    "    (label_ye, label_e), corr_result_fe, \n",
    "    title='Serial Correlation for Fixed Effects', floatfmt='.4f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRICT EXOGENEITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test if the assumption of strict exogeneity is satisfied in our model. If this is not the case, the consistency of FE, FD and RE will be flawed.\n",
    "\n",
    "We can do this by adding a lead of our explanatory variables (lemp $\\&$ lcap) to the model and use the corresponding t-values to determine significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exogeneity test\n",
      "Dependent variable: Log Deflated Sales\n",
      "\n",
      "                        Beta      Se    t-values\n",
      "--------------------  ------  ------  ----------\n",
      "Log Employment        0.6247  0.0290     21.5751\n",
      "Log Adjusted Capital  0.0576  0.0253      2.2759\n",
      "Employment Lead       0.1571  0.0296      5.3140\n",
      "Capital Lead          0.0418  0.0279      1.5014\n",
      "R² = 0.462\n",
      "σ² = 0.016\n"
     ]
    }
   ],
   "source": [
    "def exogeneity_test(X, y, T, T_list):\n",
    "\n",
    "    ## Defining the lead matrix\n",
    "    \n",
    "    F_T = np.eye(T, k=1)\n",
    "    F_T = F_T[:-1]\n",
    "\n",
    "    ## Defining leads of the explanatory variables of interest\n",
    "\n",
    "    capital_lead = lm.perm(F_T, X[:, 2].reshape(-1, 1))\n",
    "    employment_lead = lm.perm(F_T, X[:, 1].reshape(-1, 1))\n",
    "\n",
    "    ## Creating variable data for exogeneity test\n",
    "\n",
    "    X_exo = X[T_list != 1979]\n",
    "    X_exo = np.hstack((X_exo, capital_lead))\n",
    "    X_exo = np.hstack((X_exo, employment_lead))\n",
    "    y_exo = y[T_list != 1979]\n",
    "\n",
    "    ## Defining the time-demeaning matrix\n",
    "\n",
    "    Q_T = demeaning_matrix(T - 1)\n",
    "\n",
    "    ## Applying the matrix\n",
    "\n",
    "    yw_exo = lm.perm(Q_T, y_exo)\n",
    "    xw_exo = lm.perm(Q_T, X_exo)\n",
    "    xw_exo = xw_exo[:, 1:]\n",
    "\n",
    "    ## Updating the list of x-labels\n",
    "\n",
    "    label_exo = label_x_fe + ['Employment Lead'] + ['Capital Lead']\n",
    "\n",
    "    # Estimate model using the fixed effects methods\n",
    "\n",
    "    exo_test = lm.estimate(\n",
    "        yw_exo, xw_exo, T=T - 1, transform='fe'\n",
    "    )\n",
    "\n",
    "    # Printing the table containing estimation data\n",
    "\n",
    "    lm.print_table(\n",
    "        (label_y, label_exo), \n",
    "        exo_test, title='Exogeneity test', floatfmt='.4f'\n",
    "    )\n",
    "    \n",
    "exogeneity_test(X, y, T, T_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAUSMAN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier in this file, the RE method takes has advantage over FE, as the unobserved heterogeneity isn't demeaned away. Since the RE also requires another assumption in $E[c_i\\mathbf{x}_{it}] = 0$, we have to test if this assumption is met. This is what can be determined by a Hausman test.\n",
    "\n",
    "Initially, we are checking for similarity in the estimates from the two different regression models to see if the coefficients are consistent. If not, we can assume that RE.1(b) ($E[c_i\\mathbf{x}_{it}] = 0$) isn't satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing the beta estimates and the covariance matrix in variables\n",
    "\n",
    "b_re = re_result['b_hat']\n",
    "b_re = b_re[1:]\n",
    "\n",
    "cov_re = re_result['cov']\n",
    "cov_re = cov_re[1:,1:]\n",
    "\n",
    "## Defining the differences between the coefficient estimates\n",
    "\n",
    "hat_diff = fe_result['b_hat'] - b_re \n",
    "\n",
    "## Defining the differences between the covarinance matrices\n",
    "\n",
    "cov_diff = fe_result['cov'] - cov_re\n",
    "\n",
    "## Calculating the Hausman test value\n",
    "\n",
    "H = hat_diff.T@la.inv(cov_diff)@hat_diff \n",
    "\n",
    "## Computing the p-value for the Hausman test\n",
    "\n",
    "p_val = chi2.sf(H.item(), hat_diff.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  b_fe    b_re    b_diff\n",
      "------  ------  --------\n",
      "0.7069  0.7331   -0.0262\n",
      "0.1424  0.2148   -0.0724\n",
      "The Hausman test statistic is: 15.86, with p-value: 0.00.\n"
     ]
    }
   ],
   "source": [
    "## Printing a table containing Hausman test results\n",
    "\n",
    "def print_h_test(fe_result, re_result, hat_diff, p_val):\n",
    "    table = []\n",
    "    for i in range(len(hat_diff)):\n",
    "        row = [\n",
    "            fe_result['b_hat'][i], re_result['b_hat'][1:][i], hat_diff[i]\n",
    "        ]\n",
    "        table.append(row)\n",
    "\n",
    "    print(tabulate(\n",
    "        table, headers=['b_fe', 'b_re', 'b_diff'], floatfmt='.4f'\n",
    "        ))\n",
    "    print(f'The Hausman test statistic is: {H.item():.2f}, with p-value: {p_val:_.2f}.')\n",
    "print_h_test(fe_result, re_result, hat_diff, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WALD TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main interest was to analyze the dataset for constant returns to scale (CRS), which mathemathically can be shown to be the case where the exponents in the Cobb-Douglas function sums to 1.\n",
    "\n",
    "Therefore, we've chosen to use a Wald test to investigate our hypothesis about CRS. We are using a Wald test for every regression model besides FD, since FE and FD should be consistent under the exact same econometric assumptions.\n",
    "\n",
    "Our null- and alternative hypothesis is stated as:\n",
    "\n",
    "$$ H_0 : \\beta_K + \\beta_L = 1 \\tag{8}$$\n",
    "$$ H_A : \\beta_K + \\beta_L \\neq 1 \\tag{9}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method      Wald Statistic    P-value\n",
      "--------  ----------------  ---------\n",
      "POLS                4.8971   0.086420\n",
      "FE                 17.8704   0.000024\n",
      "RE                 12.6369   0.001803\n"
     ]
    }
   ],
   "source": [
    "## Defining variables for the covariance matrices and the coefficient estimates\n",
    "\n",
    "cov_ols = ols_result['cov']\n",
    "cov_fe = fe_result['cov']\n",
    "cov_re = re_result['cov']\n",
    "\n",
    "b_hat_ols = ols_result['b_hat']\n",
    "b_hat_fe = fe_result['b_hat']\n",
    "b_hat_re = re_result['b_hat']\n",
    "\n",
    "## Hypothesis matrix R for testing beta_1 + beta_2 = 1\n",
    "\n",
    "R = np.array([0, 1, 1])\n",
    "\n",
    "## Reshaping R to fit the dimensions\n",
    "\n",
    "R = np.reshape(R, (1,-1))\n",
    "\n",
    "## Hypothesized value under H0\n",
    "\n",
    "q0 = np.array([1])\n",
    "\n",
    "## Computing the linear combination of estimates\n",
    "\n",
    "q_hat_ols = np.dot(R, ols_result['b_hat'])\n",
    "q_hat_fe = np.dot(R[:,1:], fe_result['b_hat'])\n",
    "q_hat_re = np.dot(R, re_result['b_hat'])\n",
    "\n",
    "## Reshaping the linear combinations\n",
    "\n",
    "q_list = [q0, q_hat_ols, q_hat_fe, q_hat_re]\n",
    "\n",
    "for q in q_list:\n",
    "    q = np.reshape(q, (-1,1))\n",
    "\n",
    "## Compute Wald statistic for the different methods\n",
    "    \n",
    "W = (q_hat_ols - q0).T @ la.inv(R @ cov_ols @ R.T) @ (q_hat_ols - q0)\n",
    "\n",
    "W_fe = (q_hat_fe - q0).T @ la.inv(R[:,1:] @ cov_fe @ R[:,1:].T) @ (q_hat_fe - q0)\n",
    "\n",
    "W_re = (q_hat_re - q0).T @ la.inv(R @ cov_re @ R.T) @ (q_hat_re - q0)\n",
    "\n",
    "## Defining the degrees of freedoms for the different models\n",
    "\n",
    "df_ols_re = 2\n",
    "df_fe = 1\n",
    "\n",
    "## Compute p-value for every model\n",
    "\n",
    "p_value_ols = chi2.sf(W.item(), df_ols_re)\n",
    "p_value_fe = chi2.sf(W_fe.item(), df_fe)\n",
    "p_value_re = chi2.sf(W_re.item(), df_ols_re)\n",
    "\n",
    "## Creating a table containing the Wald test results\n",
    "\n",
    "test_results = [\n",
    "    [\"POLS\", f\"{W.item():.4f}\", f\"{p_value_ols:.6f}\"],\n",
    "    [\"FE\", f\"{W_fe.item():.4f}\", f\"{p_value_fe:.6f}\"],\n",
    "    [\"RE\", f\"{W_re.item():.4f}\", f\"{p_value_re:.6f}\"]\n",
    "]\n",
    "\n",
    "headers = [\"Method\", \"Wald Statistic\", \"P-value\"]\n",
    "\n",
    "print(tabulate(test_results, headers=headers, floatfmt=(\".\", \".4f\", \".6f\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Returns to Scale (CRS)\n",
    "\n",
    "At last, we have provided a simple table to show what the parameters from the different regression methods sums to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method      Beta_L + Beta_K\n",
      "--------  -----------------\n",
      "POLS                 0.9867\n",
      "RE                   0.9479\n",
      "FE                   0.8493\n",
      "FD                   0.7801\n"
     ]
    }
   ],
   "source": [
    "beta_sums = [\n",
    "    ['POLS', ols_result['b_hat'][1]+ols_result['b_hat'][2]],\n",
    "    [ 'RE' , re_result['b_hat'][1]+re_result['b_hat'][2]],\n",
    "    [ 'FE' , fe_result['b_hat'][0]+fe_result['b_hat'][1]],\n",
    "    [ 'FD' , fd_result['b_hat'][0]+fd_result['b_hat'][1]]\n",
    "]\n",
    "\n",
    "headers = [\"Method\", \"Beta_L + Beta_K\"]\n",
    "\n",
    "print(tabulate(beta_sums, headers=headers, floatfmt=\".4f\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
